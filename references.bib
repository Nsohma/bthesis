@article{tokodai-xyz2015,
    author = {東工大 太郎},
    title = {良い論文の書き方},
    journal = {Journal of XYZ},
    volume = 3,
    number = 4,
    pages = {15--34},
    year = 2015
}

@article{fucci,
  author = {渕上 正浩 and 齋藤 豪},
  journal = {芸術科学フォーラム},
  number = {10},
  title = {アニメ制作管理と同時に行う中間生成物の蓄積手法},
  volume = {49},
  year = {2025}
}


@misc{shotgrid,
  author = "AREA JAPAN",
  title = "CG・映像・ゲーム業界向けにプロジェクト・アセット管理、レビュー機能がついた管理ツール Flow Production Tracking（旧ShotGrid）",
  howpublished = "\url{https://area.autodesk.jp/product/shotgrid/}",
  year = "2024",
  note = {Accessed: 2024-11-08}
}

@misc{rv,
  author = "AUTODESK",
  title = "RV - Flow Production Tracking",
  howpublished = "\url{https://www.autodesk.com/products/flow-production-tracking/rv}",
  year = "2024",
  note = {Accessed: 2024-11-08}
}

@misc{savepoint,
  author = "MUGENUP",
  title = "SAVE POINT - セーブポイント",
  howpublished = "\url{https://mugenup.com/services/savepoint/}",
  year = "2024",
  note = {Accessed: 2024-11-08}
}

@misc{yamakawa,
  author={山川道子},
  title={アニメーションアーカイブの現状　2017},
  howpublished={\url{https://artscape.jp/study/digital-achive/10142147_1958.html}},
  year={2017},
  note = {Accessed: 2024-11-08}
}

@article{matsushita,
    author = {松下 光範 and 山西 良典},
    title = {アイデアソンによるアニメーション中間生成物の活用可能性の検討},
    journal = {紀要 アートリサーチ},
    year = {2023},
    volume = {23},
    pages = {137-143}
}

@misc{trigger,
    author = {株式会社トリガー},
    title = {},
    year = {2022},
    howpublished = {\url{https://doi.org/10.32130/idr.15.1}},
    note = {Accessed: 2024-11-08}
}

@misc{hiero,
  author = {FOUNDRY},
  title = {Hiero \& HieroPlayer | マルチショット管理ツール - Foundry},
  howpublished = {\url{https://www.foundry.com/ja/products/nuke-family/hiero}},
  year = {2024},
  note = {Accessed: 2024-11-08}
}

@misc{anikuro,
  author = {MemoRy-TeCH},
  title = {アニクロ：アニメ制作管理システム},
  howpublished = {\url{https://kaleida.jp/service/anikuro/}},
  year = {2024},
  note = {Accessed: 2024-11-08}
}

@misc{olmfmtool,
  author = {山岸 悟 and 高橋 優 and 深谷 祐太 and 木下 美紀 and 前島 謙宣},
  title = {OLM R\&D祭2022 デジタル作画のパイプライン構築 -OLM FM Tool-},
  howpublished = {\url{https://speakerdeck.com/olmdrd/matsuri2022-fmtool}},
  year = {2022},
  note = {Accessed: 2024-11-08}
}

@misc{sannjigenn,
  author = {株式会社サンジゲン},
  title = {株式会社サンジゲン},
  howpublished = {\url{https://www.sanzigen.co.jp/}},
  year = {2024},
  note = {Accessed: 2024-11-08}
}

@techreport{animereport,
    author = {一般社団法人日本動画協会},
    title = {平成 28 年度我が国におけるデータ駆動型社会に係る基盤整備（アニ
メーション分野におけるデジタル制作環境整備に係る調査研究）報告書アニメのデジタル制作導入
ガイド—日本のアニメーション制作が培ってきた技術を、未来の才能に引き継いでいくためにー},
    howpublished = {\url{https://aja.gr.jp/download/h28_anime_-digital_sisk_dny_gd}},
    year = {2017}
}

@article{animetechinsert,
  title={日本アニメ産業における情報技術導入},
  author={一小路 武安},
  journal={赤門マネジメント・レビュー},
  volume={11},
  number={6},
  pages={349-376},
  year={2012},
  doi={10.14955/amr.110601}
}

@misc{xdts,
author = {株式会社セルシス},
title = {XDTS ファイルフォーマット - CLIP STUDIO PAINT},
howpublished = {\url{https://vd.clipstudio.net/clipcontent/paint/app/ToeiAnimation/XDTSFileFormat_ja.pdf}},
year = {2018},
}

@misc{aimdb,
author = {新潟大学},
title = {新潟大学アニメ中間素材データベース AIMDB},
howpublished = {\url{https://aim.arc.niigata-u.ac.jp/}},
year = {},
}

@article{matsumoto,
  title={[A11] 実務家に聞くアニメアーカイブデータベースの可能性と課題：新潟大学アニメ中間素材データベース（AIMDB）へのフィードバックから},
  author={松本 淳},
  journal={デジタルアーカイブ学会誌},
  volume={7},
  number={s2},
  pages={s39-s42},
  year={2023},
  doi={10.24506/jsda.7.s2_s39}
}

@article{nagao,
  title={日本のアニメーション制作における各作業工程に関わるデータ事例に基づくデータベースの設計に関する研究},
  author={長尾 隣 and 齋藤 豪},
  journal={画像電子学会研究会講演予稿},
  volume={21.04},
  pages={170-173},
  year={2022},
  doi={10.11371/wiieej.21.04.0_170}
}

@inproceedings{yolo,
  author    = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  booktitle = {CVPR},
  year      = {2016}
}

@inproceedings{glip,
  author    = {Liunian Harold Li and others},
  title     = {Grounded Language-Image Pre-Training},
  booktitle = {CVPR},
  year      = {2022}
}

@inproceedings{detr,
  author    = {Nicolas Carion and others},
  title     = {End-to-End Object Detection with Transformers},
  booktitle = {ECCV},
  year      = {2020}
}

@inproceedings{deformabledetr,
  author    = {Xizhou Zhu and others},
  title     = {Deformable DETR: Deformable Transformers for End-to-End Object Detection},
  booktitle = {ICLR},
  year      = {2021}
}

@inproceedings{groundingdino,
  author    = {Shilong Liu and others},
  title     = {Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},
  booktitle = {ECCV},
  year      = {2024}
}

@inproceedings{dino,
  author = {Hao Zhang and others},
  booktitle = {ICLR},
  title = {DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection},
  year = {2023}
}

@article{mmgroundingdino,
  author  = {Xiangyu Zhao and others},
  title   = {An Open and Comprehensive Pipeline for Unified Object Grounding and Detection},
  year    = {2024}
}


@misc{mmdetection,
  author       = {OpenMMLab},
  title        = {MMDetection},
  howpublished = {\url{https://github.com/open-mmlab/mmdetection}},
  year         = {2024},
  note         = {Accessed: 2026-01-28}
}

@misc{marukome,
  author = {マルコメ株式会社},
  title = {料亭の味 | cmギャラリー | マルコメ},
  howpublished = {\url{https://www.marukome.co.jp/cm/ryotei/}},
  year = {2022},
  note = {Accessed: 2024-11-08}
}

@misc{clipstudio,
  author = "セルシス",
  title = "CLIP STUDIO PAINT",
  howpublished = "\url{https://www.clipstudio.net/ja/?gad_source=1&gclid=CjwKCAiAzc2tBhA6EiwArv-i6QF85pfLtyUNAM7eQ8bbma4H9GOG1V6cuOWRjDdWZ2YvoADH1f7hjxoCYpUQAvD_BwE}",
  year = "2024",
  note = {Accessed: 2024-11-08}
}

@misc{toei,
  author = "東映アニメーション",
  title = "東映アニメーション デジタルタイムシート",
  howpublished = "\url{https://www.clipstudio.net/ja/dl/toeianimation/}",
  year = "2024",
  note = {Accessed: 2024-11-08}
}

@misc{aftereffect,
  author = "Adobe",
  title = "Adobe After Effects",
  howpublished = "\url{https://www.adobe.com/jp/products/aftereffects.html}",
  year = "2024",
  note = {Accessed: 2024-11-08}
}

@misc{redmine,
  author = "ファーエンドテクノロジー株式会社",
  title = "Redmine.JP",
  howpublished = "\url{https://redmine.jp}",
  year = "2024",
  note = {Accessed: 2024-12-21}
}

@misc{opentoonz,
  author = "DWANGO",
  title = "openToonz",
  howpublished = "\url{https://opentoonz.github.io}",
  year = "2024",
  note = {Accessed: 2024-12-21}
}

@misc{krita,
  author = "Krita財団",
  title = "KRITA",
  howpublished = "\url{https://krita.org/ja/}",
  year = "2024",
  note = {Accessed: 2024-12-21}
}

@article{yiyi,
  title={日本のセルアニメーション制作におけるオンラインデータ管理と 映像比較を用いた制作支援に関する研究},
  author={夏 威夷 and 渕上 正浩 and 齋藤 豪},
  journal={Media Computing Conference},
  volume={},
  pages={},
  year={2023},
}

@article{mikami,
   author	 = "三上,浩司 and 安芸,淳一郎 and 宮,徹 and 金子,満",
   title	 = "アニメーション制作におけるコンピュータ活用のためのワークフローの提案と制作技術の蓄積",
   journal	 = "情報処理学会論文誌",
   year 	 = "2008",
   volume	 = "49",
   number	 = "8",
   pages	 = "2773--2782",
   month	 = "aug"
}

@misc{hakubo,
    author = {株式会社TwilightStudio},
    title ={薄暮},
    year = {2019},
    howpublished = {\url{https://www.hakubo-movie.jp/}},
    note = {Accessed: 2025-01-08}
}

@book{digitalmanual34,
  title={プロフェッショナルのためのデジタルアニメマニュアル: 2003-2004},
  author={デジタルアニメ制作技術研究会},
  url={https://books.google.co.jp/books?id=yxG2MgAACAAJ},
  year={2004},
  publisher={デジタルアニメ製作技術研究会}
}

@book{ digitalmanual5,
  author    = "東京工科大学片桐研究所クリエイティブ・ラボ",
  title     = "プロフェッショナルのためのデジタルアニメマニュアル : 工程・知識・用語",
  publisher = "東京工科大学片桐研究所クリエイティブ・ラボ",
  year      = "2005",
  URL       = "https://ci.nii.ac.jp/ncid/BA76059591"
}


%% --- LLM / Transformer ---
@inproceedings{vaswani17,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {30},
  pages     = {5998--6008},
  year      = {2017}
}

@inproceedings{brown20,
  author    = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {33},
  pages     = {1877--1901},
  year      = {2020}
}

%% --- Vision / Vision-Language ---
@inproceedings{radford21,
  author    = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  title     = {Learning Transferable Visual Models from Natural Language Supervision},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  series    = {Proceedings of Machine Learning Research (PMLR)},
  volume    = {139},
  pages     = {8748--8763},
  year      = {2021}
}

@misc{li22,
  author        = {Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
  title         = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  year          = {2022},
  eprint        = {2201.12086},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2201.12086}
}

@inproceedings{alayrac22,
  author    = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Igor Kovernikov and Alexandre Lacoste and et al.},
  title     = {Flamingo: a Visual Language Model for Few-Shot Learning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2204.14198}
}

@inproceedings{liu23,
  author    = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  title     = {Visual Instruction Tuning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2023},
  eprint        = {2304.08485},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url       = {https://arxiv.org/abs/2304.08485}
}

%% --- “ローカル運用しやすい”最新VLM（あなたの議論の中心） ---
@article{bai25,
  title         = {Qwen3-VL Technical Report},
  author        = {Shaoyang Bai and Yang Cai and Ruicheng Chen and et al.},
  year          = {2025},
  eprint        = {2511.21631},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2511.21631}
}

@misc{wang25,
  title         = {InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency},
  author        = {Weiyun Wang and Zhangwei Gao and Lixin Gu and Hengjun Pu and Long Cui and Xingguang Wei and Zhaoyang Liu and Linglin Jing and Shenglong Ye and Jie Shao and Zhaokai Wang and Zhe Chen and Hongjie Zhang and Ganlin Yang and Haomin Wang and Qi Wei and Jinhui Yin and Wenhao Li and Erfei Cui and Guanzhou Chen and Zichen Ding and Changyao Tian and Zhenyu Wu and Jingjing Xie and Zehao Li and Bowen Yang and Yuchen Duan and Xuehui Wang and Songze Li and Xiangyu Zhao and Haodong Duan and Nianchen Deng and Bin Fu and Yinan He and Yi Wang and Conghui He and Botian Shi and Junjun He and Yingtong Xiong and Han Lv and Lijun Wu and Wenqi Shao and Kaipeng Zhang and Huipeng Deng and Biqing Qi and Jiaye Ge and Qipeng Guo and Wenwei Zhang and Wanli Ouyang and Limin Wang and Min Dou and Xizhou Zhu and Tong Lu and Dahua Lin and Jifeng Dai and Bowen Zhou and Weijie Su and Kai Chen and Yu Qiao and Wenhai Wang and Gen Luo},
  year          = {2025},
  eprint        = {2508.18265},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2508.18265}
}

@misc{an25,
  title         = {LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training},
  author        = {Xiang An and Yin Xie and Kaicheng Yang and Wenkang Zhang and Xiuwei Zhao and Zheng Cheng and Yirui Wang and Songcen Xu and Changrui Chen and Chunsheng Wu and Huajie Tan and Chunyuan Li and Jing Yang and Jie Yu and Xiyao Wang and Bin Qin and Yumeng Wang and Zizhen Yan and Ziyong Feng and Ziwei Liu and Bo Li and Jiankang Deng},
  year          = {2025},
  eprint        = {2509.23661},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2509.23661}
}

@misc{han25,
  title         = {Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs},
  author        = {{Microsoft} and Abdelrahman Abouelenin and Atabak Ashfaq and Adam Atkinson and et al.},
  year          = {2025},
  eprint        = {2503.01743},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2503.01743}
}

@misc{mistral25,
  author       = {{Mistral AI}},
  title        = {Introducing Mistral 3: The next generation of open multimodal and multilingual AI},
  year         = {2025},
  month        = dec,
  day          = {2},
  url          = {https://mistral.ai/news/mistral-3},
  note         = {Online; accessed 2026-01-24}
}

@misc{sanseviero25,
  author       = {Omar Sanseviero and Philipp Schmid},
  title        = {Introducing Gemma 3: The Developer Guide},
  year         = {2025},
  month        = mar,
  day          = {12},
  url          = {https://developers.googleblog.com/en/introducing-gemma3/},
  note         = {Online; accessed 2026-01-24}
}

%% --- ベンチマーク／評価（VLM比較の根拠として使える） ---
@article{gao25,
  author  = {Junyuan Gao and Jiahe Song and Jiang Wu and Runchuan Zhu and Guanlin Shen and Shasha Wang and Xingjian Wei and Haote Yang and Songyang Zhang and Weijia Li and Bin Wang and Dahua Lin and Lijun Wu and Conghui He},
  title   = {PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model},
  journal = {CoRR},
  volume  = {abs/2503.18484},
  year    = {2025},
  url     = {https://arxiv.org/abs/2503.18484}
}

@misc{mukherjee25,
  author        = {Arka Mukherjee and S. Ghosh},
  title         = {mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models},
  year          = {2025},
  eprint        = {2511.09339},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2511.09339}
}

@misc{joshi26,
  title         = {DatBench: Discriminative, Faithful, and Efficient VLM Evaluations},
  author        = {Siddharth Joshi and Haoli Yin and Rishabh Adiga and Ricardo Monti and Aldo Carranza and Alex Fang and Alvin Deng and Amro Abbas and Brett Larsen and Cody Blakeney and Darren Teh and David Schwab and Fan Pan and Haakon Mongstad and Jack Urbanek and Jason Lee and Jason Telanoff and Josh Wills and Kaleigh Mentzer and Luke Merrick and Parth Doshi and Paul Burstein and Pratyush Maini and Scott Loftin and Spandan Das and Tony Jiang and Vineeth Dorna and Zhengping Wang and Bogdan Gaza and Ari Morcos and Matthew Leavitt},
  year          = {2026},
  eprint        = {2601.02316},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url           = {https://arxiv.org/abs/2601.02316}
}


@misc{qwen3vl_card,
  author       = {Qwen Team},
  title        = {Qwen3-VL-30B-A3B-Instruct (Model Card)},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct}},
  year         = {2025},
  note         = {Accessed: 2026-01-28}
}

@misc{internvl35_card,
  author       = {OpenGVLab},
  title        = {InternVL3.5-8B (Model Card)},
  howpublished = {\url{https://huggingface.co/OpenGVLab/InternVL3_5-8B}},
  year         = {2025},
  note         = {Accessed: 2026-01-28}
}

@misc{phi4mm_card,
  author       = {Microsoft},
  title        = {Phi-4-multimodal-instruct (Benchmarks / Model Page)},
  howpublished = {\url{https://ai.azure.com/catalog/models/phi-4-multimodal-instruct}},
  year         = {2025},
  note         = {Accessed: 2026-01-28}
}

@misc{gemma3_vision_table,
  author       = {NVIDIA},
  title        = {Vision benchmark table including Gemma 3 27B IT (Model Card / Comparison)},
  howpublished = {\url{https://build.nvidia.com/mistralai/mistral-small-3_1-24b-instruct-2503/modelcard}},
  year         = {2025},
  note         = {Accessed: 2026-01-28}
}







